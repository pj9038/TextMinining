{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석\n",
    ">영어는 띄어쓰기를 기준으로 해도 단어의 최소 의미와 비교적 일치하나 한국어의 경우 교착어 특징을 갖기 때문에 띄어쓰기가 아닌 \"형태소\"가 의미의 최소 단위이다.\n",
    ">따라서 한국어 텍스트에 대한 토큰화는 \"띄어쓰기 기준\" 아닌 `형태소`가 된다.\n",
    "이러한 이유로 한국어 텍스트 분석에서 `형태소분석`이 중요한 것은 전처리 단계 중 단어의 \"토큰화\"가 \"형태소\"에 해당되기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예제 파일을 불러와보자. 드라마 '눈이 부시게' 의 기사 일부를 발췌한 텍스트 파일을 불러온 후, 띄어쓰기 기준으로 분절한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이\", \"부시게'가\", '가뿐하게', '지상파', '월화극을', '따돌리며', '6%를', '돌파했다.', '27일', '시청률', '조사회사', '닐슨', '코리아에', '따르면', '26일', '방송된', 'JTBC', '월화극', \"'눈이\", \"부시게'는\", '6.567%(전국', '유료가구', '기준)의', '시청률을', '기록했다.', '5회', '연속', '자체', '최고', '시청률을', '찍으며', '멈출', '줄', '모르는', '상승세를', '이어가고', '있다.', '동시에', '첫', '6%대', '돌파였다.', '동', '시간대', '방송된', '지상파', '3사', '월화극', 'SBS', \"'해치'\", 'KBS', '2TV', \"'동네변호사\", '조들호2:죄와', \"벌'\", 'MBC', \"'아이템'을\", '따돌리고', '우위를', '점했다.', 'tvN', \"'왕이\", '된', \"남자'(9.5%)를\", '잇는', '월화극', '전체', '2위에', '이름을', '올렸다.', \"'왕이\", '된', \"남자'의\", '경우', '종영을', '앞두고', '있기에', \"'눈이\", \"부시게'가\", '어디까지', '상승할', '수', '있을지', '주목된다.', '이날', '방송에는', '김혜자(김혜자)가', '방송', '말미', '시간을', '되돌리는', '시계를', '발견하는', '모습이', '그려졌다.', '전무송이', '이', '시계를', '차고', '있었고', '시계를', '본', '후', '눈빛이', '심하게', '흔들린', '김혜자의', '모습을', '통해', '다시금', '시간을', '되돌릴', '수', '있을지', '여부에', '관심이', '쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "\n",
    "    text_words = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        words = line.split() # 일반적으로 단어 구분은 띄어쓰기(공백) 기준으로 이뤄지기 때문에 띄어쓰기 기준으로 분절함.\n",
    "        text_words += words\n",
    "\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위와 동일한 파일을 띄어쓰기가 아닌 줄바꿈을 기준으로 분절할 수도 있다. `split()` 이 아닌 `splitlines()`를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록했다.\", '5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다. 동시에 첫 6%대 돌파였다.', \"동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다.\", \"tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다.\", '이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다. 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "\n",
    "    text_words = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        words = line.splitlines() # 띄어쓰기가 아닌 줄바꿈 기준\n",
    "        text_words += words\n",
    "\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예를 들어, 시를 불러온다고 했을 때, 띄어쓰기보다 줄바꿈을 기준으로 가져오는 게 더 필요할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는 날까지 하늘을 우러러\n",
      "한 점 부끄럼이 없기를,\n",
      "잎새에 이는 바람에도\n",
      "나는 괴로워했다.\n",
      "['죽는 날까지 하늘을 우러러', '한 점 부끄럼이 없기를,', '잎새에 이는 바람에도', '나는 괴로워했다.']\n"
     ]
    }
   ],
   "source": [
    "poem = \"\"\"죽는 날까지 하늘을 우러러\\n한 점 부끄럼이 없기를,\\n잎새에 이는 바람에도\\n나는 괴로워했다.\"\"\"\n",
    "print(poem)\n",
    "\n",
    "poem1 = poem.splitlines()\n",
    "\n",
    "print(poem1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 토큰화\n",
    "> 단어 토큰화 전에 문서를 문장으로 분절할 수 있다. 구분자(마침표, 물음표, 느낌표 등)에 따라 문장을 구분해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 먼저 마침표에 따라 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\n",
      "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록? 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\n",
      "동시에 첫 6%대 돌파였다\n",
      "동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다\n",
      "tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다\n",
      "'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\n",
      "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다? 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata_add.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "\n",
    "    text_sentences = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        sentences = line.split(\". \")   # 일반적으로 문장 구분은 마침표(.) 기준으로 이뤄지기 때문에 마침표를 기준으로 분절함.\n",
    "        text_sentences += sentences\n",
    "\n",
    "# 결과 보기 편하게 하나씩 출력해보았다.        \n",
    "for i in range(len(text_sentences)):\n",
    "    print(text_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 마침표, 물음표, 느낌표 모든 구분자를 포함하여 구분해보자. \n",
    "* \".\",\"?\",\"!\" 를 모두 줄바꿈(\\n)으로 바꾼다음 `splitlines()` 를 이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\n",
      "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록?\n",
      "5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\n",
      "동시에 첫 6%대 돌파였다\n",
      "동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다\n",
      "tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다\n",
      "'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\n",
      "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다?\n",
      "전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata_add.txt\"\n",
    "\n",
    "text_sentences=[]\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    \n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(\". \", \"\\n\")\n",
    "        line = line.replace(\"? \", \"?\\n\")\n",
    "        line = line.replace(\"! \", \"!\\n\")   \n",
    "        sub_sentences = line.splitlines() # splitlines()메소드는 아래에서 설명\n",
    "        \n",
    "        text_sentences +=sub_sentences\n",
    "        \n",
    "# 결과 보기 편하게 하나씩 출력해보았다.        \n",
    "for i in range(len(text_sentences)):\n",
    "    print(text_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 자주 사용하는 과정이므로 사용자 함수로 재사용 가능하게 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정규식을 이용해서 동일한 함수를 만들 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   # re 모듈 호출\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(\"(?<=[.?!])\\s+\", text.strip())\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용자 함수 잘 만들어졌는지 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\n",
      "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록?\n",
      "5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\n",
      "동시에 첫 6%대 돌파였다.\n",
      "동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다.\n",
      "tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다.\n",
      "'왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다!\n",
      "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다?\n",
      "전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다.\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"textdata_add.txt\"\n",
    "\n",
    "text_sentences = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        text_sentences += sub_sentences\n",
    "for i in range(len(text_sentences)):\n",
    "    print(text_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이젠 진짜로 형태소 분석을 해보자!! 단어 하나하나에 품사를 태깅하는 과정은 간략하게 다음과 같다.\n",
    "    1. Twitter 모듈안에 있는 pos 사용자 함수를 호출\n",
    "    2. 리스트 내 튜플 형태(단어, 품사)로 출력 값 생성\n",
    "    3. 필요시 리스트의 원소인 튜플을 하나씩 접근한 후, 개별 튜플의 원소들을 각각 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pj903\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('죽는', 'Verb'), ('날', 'Noun'), ('까지', 'Josa'), ('하늘', 'Noun'), ('을', 'Josa'), ('우러러', 'Noun'), ('한', 'Verb'), ('점', 'Noun'), ('부끄럼', 'Noun'), ('이', 'Josa'), ('없기를', 'Adjective'), (',', 'Punctuation'), ('잎새', 'Noun'), ('에', 'Josa'), ('이는', 'Verb'), ('바람', 'Noun'), ('에도', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('괴로워', 'Adjective'), ('했다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_pos = twitter.pos(text) # Twitter 모듈안에 있는 pos 사용자 함수를 호출  \n",
    "print(text_pos) # 리스트 내 튜플 형태(단어, 품사)로 출력 값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는\tVerb\n",
      "날\tNoun\n",
      "까지\tJosa\n",
      "하늘\tNoun\n",
      "을\tJosa\n",
      "우러러\tNoun\n",
      "한\tVerb\n",
      "점\tNoun\n",
      "부끄럼\tNoun\n",
      "이\tJosa\n",
      "없기를\tAdjective\n",
      ",\tPunctuation\n",
      "잎새\tNoun\n",
      "에\tJosa\n",
      "이는\tVerb\n",
      "바람\tNoun\n",
      "에도\tJosa\n",
      "나\tNoun\n",
      "는\tJosa\n",
      "괴로워\tAdjective\n",
      "했다\tVerb\n",
      ".\tPunctuation\n"
     ]
    }
   ],
   "source": [
    "for word, pos in text_pos:\n",
    "    print(\"{}\\t{}\".format(word, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이번엔 morphs() 메소드와 nouns() 메소드를 적용해 보면, 결과에서 보듯 문장에서 분절된 형태소와 분절된 형태소들 가운데 명사만을 각각 리스트 내 원소로 반환시켜 준다. (텍스트 분석에서 가장 많이 사용되는 품사가 명사이기 때문에 명사만 따로 메소드 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphs() 메소드 :  ['죽는', '날', '까지', '하늘', '을', '우러러', '한', '점', '부끄럼', '이', '없기를', ',', '잎새', '에', '이는', '바람', '에도', '나', '는', '괴로워', '했다', '.']\n",
      "nouns() 메소드 : ['날', '하늘', '우러러', '점', '부끄럼', '잎새', '바람', '나']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_words = twitter.morphs(text)\n",
    "text_nouns = twitter.nouns(text)\n",
    "\n",
    "print('morphs() 메소드 : ',text_words), print('nouns() 메소드 :',text_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 품사 태깅 과정을 사용자 함수로 재사용 가능하게 만들었다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(analyzer, text):\n",
    "\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 위에서 정의한 def split_sentences 호출 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)                 # morph_anal의 출력 값 = [(word, pos)] \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 모든 과정을 합쳐서 해보자! 과정을 정리하면 다음과 같다.\n",
    "    1. 데이터 불러와서 문장 단위로 분절 (이번 예제에서는 마침표, 물음표, 느낌표 모든 구분자를 기준으로 분절)\n",
    "    2. 문장별로 단어 단어에 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('27일', 'Number'), ('시청률', 'Noun'), ('조사', 'Noun'), ('회사', 'Noun'), ('닐슨', 'Noun'), ('코리아', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('26일', 'Number'), ('방송', 'Noun'), ('된', 'Verb'), ('JTBC', 'Alpha'), ('월화', 'Noun'), ('극', 'Suffix'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('는', 'Verb'), ('6.567%', 'Number'), ('(', 'Foreign'), ('전국', 'Noun'), ('유료', 'Noun'), ('가구', 'Noun'), ('기준', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('기록', 'Noun'), ('?', 'Punctuation')], [('5회', 'Number'), ('연속', 'Noun'), ('자체', 'Noun'), ('최고', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('찍으며', 'Verb'), ('멈출', 'Verb'), ('줄', 'Noun'), ('모르는', 'Verb'), ('상승세', 'Noun'), ('를', 'Josa'), ('이', 'Determiner'), ('어가', 'Noun'), ('고', 'Josa'), ('있다', 'Adjective'), ('!', 'Punctuation')]], [[('동시', 'Noun'), ('에', 'Josa'), ('첫', 'Noun'), ('6%', 'Number'), ('대', 'Verb'), ('돌파', 'Noun'), ('였다', 'Verb'), ('.', 'Punctuation')], [('동', 'Modifier'), ('시간대', 'Noun'), ('방송', 'Noun'), ('된', 'Verb'), ('지상파', 'Noun'), ('3', 'Number'), ('사', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('SBS', 'Alpha'), (\"'\", 'Punctuation'), ('해치', 'Noun'), (\"'\", 'Punctuation'), ('KBS', 'Alpha'), ('2', 'Number'), ('TV', 'Alpha'), (\"'\", 'Punctuation'), ('동네', 'Noun'), ('변호사', 'Noun'), ('조', 'Noun'), ('들', 'Suffix'), ('호', 'Noun'), ('2', 'Number'), (':', 'Punctuation'), ('죄', 'Noun'), ('와', 'Josa'), ('벌', 'Noun'), (\"'\", 'Punctuation'), ('MBC', 'Alpha'), (\"'\", 'Punctuation'), ('아이템', 'Noun'), (\"'\", 'Punctuation'), ('을', 'Josa'), ('따돌리고', 'Verb'), ('우위', 'Noun'), ('를', 'Josa'), ('점', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')], [('tvN', 'Alpha'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'(\", 'Punctuation'), ('9.5%', 'Number'), (')', 'Foreign'), ('를', 'Noun'), ('잇는', 'Verb'), ('월화', 'Noun'), ('극', 'Suffix'), ('전체', 'Noun'), ('2', 'Number'), ('위', 'Noun'), ('에', 'Josa'), ('이름', 'Noun'), ('을', 'Josa'), ('올렸다', 'Verb'), ('.', 'Punctuation')], [(\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'\", 'Punctuation'), ('의', 'Noun'), ('경우', 'Noun'), ('종영', 'Noun'), ('을', 'Josa'), ('앞두고', 'Verb'), ('있기에', 'Adjective'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('어디', 'Noun'), ('까지', 'Josa'), ('상승', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('주목', 'Noun'), ('된다', 'Verb'), ('!', 'Punctuation')]], [[('이', 'Determiner'), ('날', 'Noun'), ('방송', 'Noun'), ('에는', 'Josa'), ('김혜자', 'Noun'), ('(', 'Punctuation'), ('김혜자', 'Noun'), (')', 'Punctuation'), ('가', 'Verb'), ('방송', 'Noun'), ('말미', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌리는', 'Verb'), ('시계', 'Noun'), ('를', 'Josa'), ('발견', 'Noun'), ('하는', 'Verb'), ('모습', 'Noun'), ('이', 'Josa'), ('그려졌다', 'Verb'), ('?', 'Punctuation')], [('전무송', 'Noun'), ('이', 'Josa'), ('이', 'Noun'), ('시계', 'Noun'), ('를', 'Josa'), ('차고', 'Noun'), ('있었고', 'Adjective'), ('시계', 'Noun'), ('를', 'Josa'), ('본', 'Verb'), ('후', 'Noun'), ('눈빛', 'Noun'), ('이', 'Josa'), ('심하게', 'Adjective'), ('흔들린', 'Verb'), ('김혜자', 'Noun'), ('의', 'Josa'), ('모습', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('다시금', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌릴', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('여부', 'Noun'), ('에', 'Josa'), ('관심', 'Noun'), ('이', 'Josa'), ('쏠렸다', 'Verb'), ('.', 'Punctuation')]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pj903\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_pos(analyzer, text):\n",
    "\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 위에서 정의한 def split_sentences 호출 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)            # word와 pos가 출력 \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals\n",
    "\n",
    "# main \n",
    "\n",
    "input_file_name = r\"textdata_add.txt\"\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "textdata_pos = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        words_pos = get_pos(twitter, line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        textdata_pos.append(words_pos)\n",
    "        \n",
    "print(textdata_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 별로 잘 나뉜건지 보기 편하게 바꿔봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]]\n",
      "\n",
      "[[('27일', 'Number'), ('시청률', 'Noun'), ('조사', 'Noun'), ('회사', 'Noun'), ('닐슨', 'Noun'), ('코리아', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('26일', 'Number'), ('방송', 'Noun'), ('된', 'Verb'), ('JTBC', 'Alpha'), ('월화', 'Noun'), ('극', 'Suffix'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('는', 'Verb'), ('6.567%', 'Number'), ('(', 'Foreign'), ('전국', 'Noun'), ('유료', 'Noun'), ('가구', 'Noun'), ('기준', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('기록', 'Noun'), ('?', 'Punctuation')], [('5회', 'Number'), ('연속', 'Noun'), ('자체', 'Noun'), ('최고', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('찍으며', 'Verb'), ('멈출', 'Verb'), ('줄', 'Noun'), ('모르는', 'Verb'), ('상승세', 'Noun'), ('를', 'Josa'), ('이', 'Determiner'), ('어가', 'Noun'), ('고', 'Josa'), ('있다', 'Adjective'), ('!', 'Punctuation')]]\n",
      "\n",
      "[[('동시', 'Noun'), ('에', 'Josa'), ('첫', 'Noun'), ('6%', 'Number'), ('대', 'Verb'), ('돌파', 'Noun'), ('였다', 'Verb'), ('.', 'Punctuation')], [('동', 'Modifier'), ('시간대', 'Noun'), ('방송', 'Noun'), ('된', 'Verb'), ('지상파', 'Noun'), ('3', 'Number'), ('사', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('SBS', 'Alpha'), (\"'\", 'Punctuation'), ('해치', 'Noun'), (\"'\", 'Punctuation'), ('KBS', 'Alpha'), ('2', 'Number'), ('TV', 'Alpha'), (\"'\", 'Punctuation'), ('동네', 'Noun'), ('변호사', 'Noun'), ('조', 'Noun'), ('들', 'Suffix'), ('호', 'Noun'), ('2', 'Number'), (':', 'Punctuation'), ('죄', 'Noun'), ('와', 'Josa'), ('벌', 'Noun'), (\"'\", 'Punctuation'), ('MBC', 'Alpha'), (\"'\", 'Punctuation'), ('아이템', 'Noun'), (\"'\", 'Punctuation'), ('을', 'Josa'), ('따돌리고', 'Verb'), ('우위', 'Noun'), ('를', 'Josa'), ('점', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')], [('tvN', 'Alpha'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'(\", 'Punctuation'), ('9.5%', 'Number'), (')', 'Foreign'), ('를', 'Noun'), ('잇는', 'Verb'), ('월화', 'Noun'), ('극', 'Suffix'), ('전체', 'Noun'), ('2', 'Number'), ('위', 'Noun'), ('에', 'Josa'), ('이름', 'Noun'), ('을', 'Josa'), ('올렸다', 'Verb'), ('.', 'Punctuation')], [(\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'\", 'Punctuation'), ('의', 'Noun'), ('경우', 'Noun'), ('종영', 'Noun'), ('을', 'Josa'), ('앞두고', 'Verb'), ('있기에', 'Adjective'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('어디', 'Noun'), ('까지', 'Josa'), ('상승', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('주목', 'Noun'), ('된다', 'Verb'), ('!', 'Punctuation')]]\n",
      "\n",
      "[[('이', 'Determiner'), ('날', 'Noun'), ('방송', 'Noun'), ('에는', 'Josa'), ('김혜자', 'Noun'), ('(', 'Punctuation'), ('김혜자', 'Noun'), (')', 'Punctuation'), ('가', 'Verb'), ('방송', 'Noun'), ('말미', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌리는', 'Verb'), ('시계', 'Noun'), ('를', 'Josa'), ('발견', 'Noun'), ('하는', 'Verb'), ('모습', 'Noun'), ('이', 'Josa'), ('그려졌다', 'Verb'), ('?', 'Punctuation')], [('전무송', 'Noun'), ('이', 'Josa'), ('이', 'Noun'), ('시계', 'Noun'), ('를', 'Josa'), ('차고', 'Noun'), ('있었고', 'Adjective'), ('시계', 'Noun'), ('를', 'Josa'), ('본', 'Verb'), ('후', 'Noun'), ('눈빛', 'Noun'), ('이', 'Josa'), ('심하게', 'Adjective'), ('흔들린', 'Verb'), ('김혜자', 'Noun'), ('의', 'Josa'), ('모습', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('다시금', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌릴', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('여부', 'Noun'), ('에', 'Josa'), ('관심', 'Noun'), ('이', 'Josa'), ('쏠렸다', 'Verb'), ('.', 'Punctuation')]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(textdata_pos)):\n",
    "    print(textdata_pos[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
